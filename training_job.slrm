#!/bin/bash
#SBATCH --job-name=MedSegDiff
#SBATCH --time=48:00:00
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err
#SBATCH --ntasks=1               # Number of tasks (processes)
#SBATCH --gres=gpu:a40:1             # Request 1 GPUs
#SBATCH --cpus-per-task=4        # CPU cores per task
#SBATCH --mem=32G                # Memory per node
#SBATCH --mail-type=ALL     # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=ahmad.rm0067@gmail.com #  email

##SBATCH --partition=t4v2
#SBATCH --qos=long

# Append is important because otherwise preemption resets the file
#SBATCH --open-mode=append

echo "$(date): Job $SLURM_JOB_ID is allocated resource"

# Use source activate for older conda installations or conda init has not been set
# Ensure the path to the environment is correct
source /pkgs/anaconda3/bin/activate /h/ahmad/ShadiProject2 || conda activate /h/ahmad/ShadiProject2


# Set up checkpoint directory
CHECKPOINT_DIR=/checkpoint/${USER}

# running:
python /h/ahmad/ShadiProject/MedSegDiff/scripts/segmentation_train.py \
  --data_name ISIC \
  --data_dir "/h/ahmad/ShadiProject/ISIC" \
  --out_dir ${CHECKPOINT_DIR} \
  --image_size 256 \
  --num_channels 128 \
  --class_cond False \
  --num_res_blocks 2 \
  --num_heads 1 \
  --learn_sigma True \
  --use_scale_shift_norm False \
  --attention_resolutions 16 \
  --diffusion_steps 1000 \
  --noise_schedule linear \
  --rescale_learned_sigmas False \
  --rescale_timesteps False \
  --lr 1e-4 \
  --batch_size 8



echo `date`: "Job $SLURM_JOB_ID finished running, exit code: $?"

date=$(date '+%Y-%m-%d')
archive=$HOME/finished_jobs/$date/$SLURM_JOB_ID
mkdir -p $archive

cp ./$SLURM_JOB_ID.out $archive/job.out
cp ./$SLURM_JOB_ID.err $archive/job.err

# Copy the results from the checkpoint directory to the permanent results directory
mkdir /h/ahmad/ShadiProject/results/training_resluts/2nd_training
cp -r ${CHECKPOINT_DIR}/* /h/ahmad/ShadiProject/results/training_resluts/2nd_training
(base) ahmad@v3:~/ShadiProject/MedSegDiff$ nano launch_job_single_gpu.slrm 


